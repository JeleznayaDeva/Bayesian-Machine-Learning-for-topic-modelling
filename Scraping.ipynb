{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Twitter with snscrape\n",
    "\n",
    "This noteboook is a complimentary notebook to the course project notebook for ST451.\n",
    "\n",
    "It contains functions that are used for scraping Twitter via `snscrape`.\n",
    "\n",
    "First, we installed the package `snscrape` from its GitHub page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install git+https://github.com/JustAnotherArchivist/snscrape.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def queries(hashtag,year,num_months):\n",
    "    queries = []\n",
    "    for mon in range(num_months):\n",
    "        #random_triple = random.sample(range(28), num_days)\n",
    "        #len_mon = 30 if (mon== 0,2)\n",
    "        for day in range(28):\n",
    "            date = np.datetime64(f'{year}-{str(mon+1).zfill(2)}-{str(day+1).zfill(2)}')\n",
    "            query = f'#{hashtag} since:{date} until:{date+1}'\n",
    "            queries.append((hashtag,query))\n",
    "    return(queries)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scraper_f(query):\n",
    "    tweets_list = []\n",
    "    try:\n",
    "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper(query[1]).get_items()):\n",
    "            if i >= 100:\n",
    "                break\n",
    "            tweets_list.append((query[0],tweet))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return(tweets_list)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1619279384.801274\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import time\n",
    "\n",
    "print(time.time())\n",
    "list_of_lists = [queries(\"SuezCanal\",year,12) for year in range(2018,2022)]\n",
    "queries_list = [element for list_queries in list_of_lists for element in list_queries]\n",
    "daytweets_lists = list(map(scraper_f,queries_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1619281163.75984\n"
     ]
    }
   ],
   "source": [
    "pickle.dump( daytweets_lists, open( f'Suez_Canal_2018-2021.p', \"wb\" ) )\n",
    "print(time.time())\n",
    "#time.sleep(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time taken to implement the code in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.649309432506563\n"
     ]
    }
   ],
   "source": [
    "t = (1619281163.75984-1619279384.801274)/60\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below code is for creating a csv file with tweets\n",
    "def en_tweets_info(file_name): \n",
    "\n",
    "    daytweets = pickle.load(open(file_name,\"rb\"))\n",
    "    daytweets_flat = [element for a_list in daytweets for element in a_list]\n",
    "    tweetslist = [[tweet_tuple[0], \n",
    "                   tweet_tuple[1].date, tweet_tuple[1].id, #tweet information\n",
    "                   tweet_tuple[1].content, tweet_tuple[1].lang,\n",
    "                   tweet_tuple[1].user.username, tweet_tuple[1].user.location] \n",
    "                  for tweet_tuple in daytweets_flat if tweet_tuple[1].lang ==\"en\"]\n",
    "    \n",
    "    return tweetslist\n",
    "\n",
    "tweetsdf = pd.DataFrame([x[3] for x in en_tweets_info('Suez_Canal_2018-2021.p')], \n",
    "                                     columns = ['Content']) # just need the column with contents\n",
    "tweetslists = tweetsdf.to_csv(\"tweets.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
